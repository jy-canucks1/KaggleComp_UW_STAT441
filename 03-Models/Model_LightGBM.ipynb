{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from EDA_script_original2 import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train, df_y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a range of values for the hyperparameters to search through\n",
    "param_grid = {\n",
    "    'num_leaves': [10,30,40],\n",
    "    'learning_rate': [0.04],\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [7],\n",
    "    'lambda_l1': [0.04],\n",
    "    'lambda_l2': [0.01],\n",
    "    'min_data_in_leaf': [10],\n",
    "    'min_gain_to_split': [0.02],\n",
    "    'feature_fraction': [0.1],\n",
    "    'bagging_fraction': [0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864646\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197645\n",
      "[LightGBM] [Debug] init for col-wise cost 0.003287 seconds, init for row-wise cost 0.011353 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3945\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.280244\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216451\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.862612\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.199900\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002754 seconds, init for row-wise cost 0.012528 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3947\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579569\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606281\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864606\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197632\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002267 seconds, init for row-wise cost 0.009559 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3946\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579569\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606281\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864701\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197690\n",
      "[LightGBM] [Debug] init for col-wise cost 0.001649 seconds, init for row-wise cost 0.010232 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3948\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 5\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.865071\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197738\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002157 seconds, init for row-wise cost 0.009889 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3959\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 6\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864646\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197645\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002634 seconds, init for row-wise cost 0.009501 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3945\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.280244\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216451\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.862612\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.199900\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002814 seconds, init for row-wise cost 0.009027 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3947\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579569\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606281\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864606\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197632\n",
      "[LightGBM] [Debug] init for col-wise cost 0.003020 seconds, init for row-wise cost 0.009014 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3946\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579569\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606281\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864701\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197690\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002838 seconds, init for row-wise cost 0.009077 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3948\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.865071\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197738\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002701 seconds, init for row-wise cost 0.009905 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3959\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864646\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197645\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002713 seconds, init for row-wise cost 0.009095 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3945\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.280244\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216451\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.862612\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.199900\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002680 seconds, init for row-wise cost 0.009757 seconds\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 3947\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579569\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606281\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864606\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197632\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002033 seconds, init for row-wise cost 0.010849 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3946\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579569\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606281\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.864701\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197690\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002850 seconds, init for row-wise cost 0.009827 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010225 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3948\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] early_stopping_round is set=10, early_stopping_rounds=10 will be ignored. Current value: early_stopping_round=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.04, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.1, subsample=1.0 will be ignored. Current value: bagging_fraction=0.1\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromSparseFeatures: sparse rate 0.865071\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.197738\n",
      "[LightGBM] [Debug] init for col-wise cost 0.002280 seconds, init for row-wise cost 0.012515 seconds\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3959\n",
      "[LightGBM] [Info] Number of data points in the train set: 26880, number of used features: 317\n",
      "[LightGBM] [Info] Start training from score -4.282936\n",
      "[LightGBM] [Info] Start training from score -1.579749\n",
      "[LightGBM] [Info] Start training from score -1.216326\n",
      "[LightGBM] [Info] Start training from score -1.261657\n",
      "[LightGBM] [Info] Start training from score -1.606095\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 40 and depth = 7\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 15 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n    self._Booster = train(\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\engine.py\", line 302, in train\n    cb(callback.CallbackEnv(model=booster,\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\callback.py\", line 256, in _callback\n    _init(env)\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\callback.py\", line 226, in _init\n    raise ValueError('For early stopping, '\nValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mlgb_classifier, param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m     12\u001b[0m                            scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_log_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data to search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters and their values\u001b[39;00m\n\u001b[0;32m     20\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    945\u001b[0m     )\n\u001b[1;32m--> 947\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 15 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n    self._Booster = train(\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\engine.py\", line 302, in train\n    cb(callback.CallbackEnv(model=booster,\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\callback.py\", line 256, in _callback\n    _init(env)\n  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\callback.py\", line 226, in _init\n    raise ValueError('For early stopping, '\nValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store the best hyperparameters and their values\n",
    "best_hyperparameters = {}\n",
    "best_values = {}\n",
    " \n",
    "# Initialize the LightGBM classifier\n",
    "lgb_classifier = lgb.LGBMClassifier(objective='multiclass', num_class=5, boosting_type='gbdt', metric = 'multi_logloss', \n",
    "                                    early_stopping_rounds=10, verbosity = 2)\n",
    " \n",
    "\n",
    "# Initialize GridSearchCV for hyperparameters\n",
    "grid_search = GridSearchCV(estimator=lgb_classifier, param_grid=param_grid,\n",
    "                           scoring='neg_log_loss', cv=5)\n",
    " \n",
    "\n",
    "# Fit the model to the training data to search for the best hyperparameters\n",
    "grid_search.fit(X_train, np.ravel(y_train))\n",
    " \n",
    "\n",
    "# Get the best hyperparameters and their values\n",
    "best_params = grid_search.best_params_\n",
    "best_hyperparameters = list(best_params.keys())\n",
    "best_values = list(best_params.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mbest_params\u001b[49m)\n\u001b[0;32m      2\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "source": [
    "best_model = lgb.LGBMClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(df_test)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = log_loss(df_test, y_pred)\n",
    "lgb.plot_metric(best_model)\n",
    "print('Best hyperparameters:', best_hyperparameters)\n",
    "print('Best values:', best_values)\n",
    "print(f'Accuracy with best hyperparameters: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
